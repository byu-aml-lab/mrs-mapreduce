.. _user_manual:

**********
User Guide
**********

.. _user_manual:


Structure of a Mrs Program
==========================

A Mrs program is a user-specified class that is passed to ``mrs.main``.
Although the ``mrs.MapReduce`` class implements some common functionality, a
MapReduce program is not required to inherit from this class.  Any class that
specifies ``__init__`` and ``run`` methods with the appropriate signatures is
a valid Mrs program.  These two required methods are:

- ``__init__(self, opts, args)`` (mandatory)

    The ``__init__`` method is called with an ``opts`` object (created by
    ``optparse``) and an ``args`` list.  These arguments are created on the
    master, but the ``__init__`` method is called once on each slave (with
    identical arguments).

- ``run(self, job)`` (mandatory)

    The role of the ``run`` method is to submit datasets for evaluation and
    process their results.  It is called with a ``job`` object, an instance of
    ``mrs.job.Job``, which provides methods such as ``local_data``,
    ``file_data``, ``map_data``, and ``reduce_data`` for creating dataset
    objects and submitting them for asynchronous evaluation.  The job also
    provides ``wait`` and ``progress`` methods for determining whether
    datasets are complete.

Additional optional methods are:

- ``update_parser(cls, parser)`` (class method)

    The optional ``update_parser`` method, which must be a class method if
    specified, adds custom command-line arguments to the given option parser
    (``optparse.OptionParser``).

- ``partition(self, x, n)``

    If specified, the ``partition`` method serves as the default partition
    function for map and reduce datasets.

- ``bypass(self)``

    If the ``Bypass`` implementation is specified on the command line, then
    Mrs invokes the `bypass` method instead of the ``run`` method.  This
    provides a convenient mechanism for including serial and parallel
    implementations of a program side by side.

- ``serializer(self, key)``

    If any map or reduce function specifies a custom serializer, then the
    ``serializer`` method is called to translate the serializer's name (which
    is transmitted over the network) to the corresponding ``Serializer``
    object (which serializes and deserializes data).


The Default MapReduce Class
===========================

The ``mrs.MapReduce`` class in the file ``mrs/mapreduce.py`` provides simple
default implementations for each of the methods used by Mrs.  It serves both
as a reasonable default and also as an example.  Programs can optionally
inherit from this class and override any functions as needed.

In its basic form, the ``mrs.MapReduce`` class assumes that a MapReduce
program consists of a single map phase followed by a single reduce phase.  One
or more filenames specified on the command-line are used to read text files to
input to the map function, and a path provided on the command-line indicates
the directory for output using a human-readable file format.

The following additional methods can be overridden in a minimal MapReduce
program that uses the ``mrs.MapReduce`` class:

- ``map(self, key, value)``

    A generator which takes a key and a value and yields one or more key-value
    pairs.  See the MapReduce paper for more information about the role of a
    map function.

- ``reduce(self, key, values)``

    A generator which takes a key and a value iterator and yields one or more
    values associated with the key.  See the MapReduce paper for more
    information about the role of a reduce function.

- ``input_data(self, job)``

    Returns a dataset object that is used as the input to the map dataset.  By
    default, it uses filenames specified on the command-line (all of the
    positional arguments except the last).

- ``output_dir(self)``

    Returns a path that is used for the output directory of the reduce
    dataset.  By default, it uses a path specified on the command-line (the
    last positional arguments).

The following list details the default behavior for some of the methods
provided by the ``mrs.MapReduce`` class:

- ``update_parser(cls, parser)`` (class method)

    Modifies and returns the given option parser (from the ``optparse``
    module).  By default, it adds a simple usage statement.  Many programs add
    command-line options or change the usage statement.

- ``run(self, job)``

    Submits datasets to the ``mrs.job.Job`` and optionally waits for and
    processes the completed datasets.  By default, it assumes that there is a
    single map dataset using the ``map`` method and a single reduce dataset
    using the ``reduce`` method.  The input to the map dataset is given by
    ``input_data``, and the output from the reduce dataset is written using
    the human-readable ``TextWriter`` file format to the directory specified
    by ``output_dir``.  A program that is more complex than a simple text
    processor can override the ``run`` method to add any sequence of map and
    reduce datasets.

Of course, any of these methods may be overridden to customize behavior.


The IterativeMR Class
=====================

The ``IterativeMR`` class implements a ``run`` method that gives a
producer-consumer interface for creating iterative MapReduce programs.  In
this model, datasets are pipelined.  The ``producer`` method is called when
the number of runnable datasets is low, and the ``consumer`` method is called
when each dataset completes.  This model reduces complexity particularly for
programs where processing on the master occurs concurrently with map and
reduce tasks (for example, programs with convergence checks).


Examples
========

The ``examples`` directory includes several examples of MapReduce
programs of varying levels of complexity:

- `wordcount2.py <http://code.google.com/p/mrs-mapreduce/source/browse/examples/wordcount2.py>`_:
  a more full-featured version of WordCount that uses a combiner and
  explicitly sets a serializer (instead of automatically using pickle).

- `pi/pure_pi.py <http://code.google.com/p/mrs-mapreduce/source/browse/examples/pi/pure_pi.py>`_:
  a sample program that estimates pi by sampling a large number of values
  from a `Halton sequence <http://en.wikipedia.org/wiki/Halton_sequence>`_
  (comparable to Hadoop's ``PiEstimator`` example).  This program includes an
  example of a custom ``run`` method.

- `pi/c_pi.py <http://code.google.com/p/mrs-mapreduce/source/browse/examples/pi/c_pi.py>`_:
  a fast implementation of the Pi estimator that uses ``ctypes`` to call a
  function written in C.

- `dependency_parse.py <http://code.google.com/p/mrs-mapreduce/source/browse/examples/dependency_parse.py>`_:
  a real-life problem which unfortunately relies on some external modules.
